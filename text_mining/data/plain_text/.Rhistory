# variables, functions, and objects all variables in
# the code begin with the "var_" prefix.
#
# (2) In order to facilitate distinguishing a
# a variable's type or class, all variables are
# names using a _suffix that identifies the
# variable type.
#
# (3) In order to facilitate distinguishing between
# variables, functions, and objects all objects in
# the code begin with the "obj_" prefix.
#
# (4) Locally defined functions begin with the
# function_ prefix
#
# Copyright Carl G. Stahmer
# Director of Digital Scholarship - UC Davis Library
# Associate Director for Humanities - Data Science Initiative
# Associate Director - English Broadside Ballad Archive
#
# Portions of this code are based on Matt Jockers'
# Introduction to text analysis with R:
#
# Jockers, M. (2014).
# _Text Analysis with R for Students of Literature_
# Quantitative Methods in the Humanities and Social ….
# doi:10.1007/978-3-319-03164-4
#
# This work is licensed under a Creative Commons
# Attribution-ShareAlike 4.0 International License.
#
# see http://creativecommons.org/licenses/by-sa/4.0/
# install.packages("rJava")
# install.packages("NLP")
# install.packages("openNLP")
# install.packages("RWeka")
# install.packages("stats")
# install.packages("openNLPmodels.en", repos = "http://datacube.wu.ac.at/", type = "source")
# For alternate language openNLPModels see http://datacube.wu.ac.at/src/contrib/
library(rJava)
library(NLP)
library(openNLP)
library(RWeka)
library(stats)
###################################
#         configuration           #
###################################
# set working directory
setwd("~/Documents/rstudio_workspace/digitalmethods/text_mining/data/plainText")
# set the entity type you want to extract.  Legal
# values are:
#     date
#     location
#     money
#     organization
#     percentage
#     person
#     misc [proper nouns deemed not to fit anther category]
var_entityType_string = "person"
# set the file path
var_filePath_string = "emerson.txt"
###################################
#      function declarations      #
###################################
# A callable function that writes out the contents
# of a vector in human readable form.
function_show_vector <- function(var_file_name_vec) {
for(i in 1:length(var_file_name_vec)) {
cat(i, var_file_name_vec[i], "\n", sep=" ")
}
}
# A callable function that extracts entities
# of an identified kind from an
# AnnotatedPlainTextDocument
function_extractEntities <- function(obj_doc, var_kind_character) {
var_content_string <- obj_doc$content
obj_annotations <- annotations(obj_doc)[[1]]
if(hasArg(var_kind_character)) {
var_kindFeatures_list <- sapply(obj_annotations$features, `[[`, "kind")
var_content_string[obj_annotations[var_kindFeatures_list == var_kind_character]]
} else {
var_content_string[obj_annotations[obj_annotations$type == "entity"]]
}
}
# A callable function that checks to see
# if an annotated document has any matching
# entities
function_checkForEntities <- function(doc, kind) {
#s <- doc$content
a <- annotations(doc)[[1]]
boolean_return <- TRUE
if(hasArg(kind)) {
k <- sapply(a$features, `[[`, "kind")
temp <- a[k == kind]
size <- length(temp)
if (size < 1) {
boolean_return <- FALSE
}
} else {
boolean_return <- FALSE
}
return(boolean_return)
}
###################################
#        Operational Code         #
###################################
# load the file to be analized into a character vector
# the resulting vector will have as many elements as
# lines in the file with the contents of each line
# contained in a character vector.
text.vector <- readLines(var_filePath_string)
# collapse the vector of lines into a single character
# vector
text.vector <- paste(text.vector, collapse = " ")
# explicitly convert the text.string character to a
# string class.  Necessary because the NLP is written
# in java.
text.string <- as.String(text.vector)
# create the annotators.
sent_ann <- Maxent_Sent_Token_Annotator()
word_ann <- Maxent_Word_Token_Annotator()
my.ann <- Maxent_Entity_Annotator(kind = var_entityType_string)
# assemble the list of annotators to
# send to the annotate function
pipeline.list <- list(sent_ann, word_ann, my.ann)
# create the final annotator model
text.annotator <- annotate(text.string, pipeline.list)
# create an annotated doc.  This is a version of the document that is
# represented as a structured hierarchy of sentences and words
text.doc <- AnnotatedPlainTextDocument(text.string, text.annotator)
# check to see if there are any matching entities.
# If so, process. If not, extit.
if (function_checkForEntities(text.doc, kind = var_entityType_string)) {
# get all entities of the type we are looking for
debugonce(function_extractEntities)
foundEntities.v <- function_extractEntities(text.doc, var_kind_character = var_entityType_string)
# get vector of unique items
uniqueEntities.v <- unique(foundEntities.v)
# sort the entities vector
sortedEntities.v <- sort(uniqueEntities.v)
print("Sorted entity list:")
function_show_vector(sortedEntities.v)
###################################
# Code below is for cleaning list #
###################################
# review list of returned entities and create list
# of items entities that should be removed from the list
droplist.v <- c("With", "Starbuck", "Watts", "Plato")
# now remove items from the droplist from the vector of
# extracted named entities
finalNameList.v <- sortedEntities.v[! sortedEntities.v %in% droplist.v ]
print("Filtered entity list:")
function_show_vector(finalNameList.v)
} else {
print("No entities found")
}
class(var_content_string)
# Script: entity_annotator.R
#
# A script written and distributed as a teaching
# aid for demonstrating how to perform entity
# extraction in R.  The script reads a single file
# and extracts various entities (such as name, place,
# dates, etc.) from the text based on configuration.
#
# Because the code is designed for teaching, it
# aims for step by step clarity rather than code
# efficiency.  Experienced programmers will see
# many ways that this code could be made more
# efficient and elegant in terms of both processing
# speed and memory management.  The code as
# presented is designed to allow a novice coder
# to follow the logic of the script as intuitively
# as possible. With this in mind, the following
# conventions are used throught the code:
#
# (1) In order to facilitate distinguishing between
# variables, functions, and objects all variables in
# the code begin with the "var_" prefix.
#
# (2) In order to facilitate distinguishing a
# a variable's type or class, all variables are
# names using a _suffix that identifies the
# variable type.
#
# (3) In order to facilitate distinguishing between
# variables, functions, and objects all objects in
# the code begin with the "obj_" prefix.
#
# (4) Locally defined functions begin with the
# function_ prefix
#
# Copyright Carl G. Stahmer
# Director of Digital Scholarship - UC Davis Library
# Associate Director for Humanities - Data Science Initiative
# Associate Director - English Broadside Ballad Archive
#
# Portions of this code are based on Matt Jockers'
# Introduction to text analysis with R:
#
# Jockers, M. (2014).
# _Text Analysis with R for Students of Literature_
# Quantitative Methods in the Humanities and Social ….
# doi:10.1007/978-3-319-03164-4
#
# This work is licensed under a Creative Commons
# Attribution-ShareAlike 4.0 International License.
#
# see http://creativecommons.org/licenses/by-sa/4.0/
# install.packages("rJava")
# install.packages("NLP")
# install.packages("openNLP")
# install.packages("RWeka")
# install.packages("stats")
# install.packages("openNLPmodels.en", repos = "http://datacube.wu.ac.at/", type = "source")
# For alternate language openNLPModels see http://datacube.wu.ac.at/src/contrib/
library(rJava)
library(NLP)
library(openNLP)
library(RWeka)
library(stats)
###################################
#         configuration           #
###################################
# set working directory
setwd("~/Documents/rstudio_workspace/digitalmethods/text_mining/data/plainText")
# set the entity type you want to extract.  Legal
# values are:
#     date
#     location
#     money
#     organization
#     percentage
#     person
#     misc [proper nouns deemed not to fit anther category]
var_entityType_string = "person"
# set the file path
var_filePath_string = "emerson.txt"
###################################
#      function declarations      #
###################################
# A callable function that writes out the contents
# of a vector in human readable form.
function_show_vector <- function(var_file_name_vec) {
for(i in 1:length(var_file_name_vec)) {
cat(i, var_file_name_vec[i], "\n", sep=" ")
}
}
# A callable function that extracts entities
# of an identified kind from an
# AnnotatedPlainTextDocument
function_extractEntities <- function(obj_doc, var_kind_character) {
var_content_string <- obj_doc$content
obj_annotations <- annotations(obj_doc)[[1]]
if(hasArg(var_kind_character)) {
var_kindFeatures_list <- sapply(obj_annotations$features, `[[`, "kind")
var_content_string[obj_annotations[var_kindFeatures_list == var_kind_character]]
} else {
var_content_string[obj_annotations[obj_annotations$type == "entity"]]
}
}
# A callable function that checks to see
# if an annotated document has any matching
# entities
function_checkForEntities <- function(doc, kind) {
#s <- doc$content
a <- annotations(doc)[[1]]
boolean_return <- TRUE
if(hasArg(kind)) {
k <- sapply(a$features, `[[`, "kind")
temp <- a[k == kind]
size <- length(temp)
if (size < 1) {
boolean_return <- FALSE
}
} else {
boolean_return <- FALSE
}
return(boolean_return)
}
###################################
#        Operational Code         #
###################################
# load the file to be analized into a character vector
# the resulting vector will have as many elements as
# lines in the file with the contents of each line
# contained in a character vector.
text.vector <- readLines(var_filePath_string)
# collapse the vector of lines into a single character
# vector
text.vector <- paste(text.vector, collapse = " ")
# explicitly convert the text.string character to a
# string class.  Necessary because the NLP is written
# in java.
text.string <- as.String(text.vector)
# create the annotators.
sent_ann <- Maxent_Sent_Token_Annotator()
word_ann <- Maxent_Word_Token_Annotator()
my.ann <- Maxent_Entity_Annotator(kind = var_entityType_string)
# assemble the list of annotators to
# send to the annotate function
pipeline.list <- list(sent_ann, word_ann, my.ann)
# create the final annotator model
text.annotator <- annotate(text.string, pipeline.list)
# create an annotated doc.  This is a version of the document that is
# represented as a structured hierarchy of sentences and words
text.doc <- AnnotatedPlainTextDocument(text.string, text.annotator)
# check to see if there are any matching entities.
# If so, process. If not, extit.
if (function_checkForEntities(text.doc, kind = var_entityType_string)) {
# get all entities of the type we are looking for
# debugonce(function_extractEntities)
foundEntities.v <- function_extractEntities(text.doc, var_kind_character = var_entityType_string)
# get vector of unique items
uniqueEntities.v <- unique(foundEntities.v)
# sort the entities vector
sortedEntities.v <- sort(uniqueEntities.v)
print("Sorted entity list:")
function_show_vector(sortedEntities.v)
###################################
# Code below is for cleaning list #
###################################
# review list of returned entities and create list
# of items entities that should be removed from the list
droplist.v <- c("With", "Starbuck", "Watts", "Plato")
# now remove items from the droplist from the vector of
# extracted named entities
finalNameList.v <- sortedEntities.v[! sortedEntities.v %in% droplist.v ]
print("Filtered entity list:")
function_show_vector(finalNameList.v)
} else {
print("No entities found")
}
class foundEntities.v
class(foundEntities.v)
length(foundEntities.v)
# Script: entity_annotator.R
#
# A script written and distributed as a teaching
# aid for demonstrating how to perform entity
# extraction in R.  The script reads a single file
# and extracts various entities (such as name, place,
# dates, etc.) from the text based on configuration.
#
# Because the code is designed for teaching, it
# aims for step by step clarity rather than code
# efficiency.  Experienced programmers will see
# many ways that this code could be made more
# efficient and elegant in terms of both processing
# speed and memory management.  The code as
# presented is designed to allow a novice coder
# to follow the logic of the script as intuitively
# as possible. With this in mind, the following
# conventions are used throught the code:
#
# (1) In order to facilitate distinguishing between
# variables, functions, and objects all variables in
# the code begin with the "var_" prefix.
#
# (2) In order to facilitate distinguishing a
# a variable's type or class, all variables are
# names using a _suffix that identifies the
# variable type.
#
# (3) In order to facilitate distinguishing between
# variables, functions, and objects all objects in
# the code begin with the "obj_" prefix.
#
# (4) Locally defined functions begin with the
# function_ prefix
#
# Copyright Carl G. Stahmer
# Director of Digital Scholarship - UC Davis Library
# Associate Director for Humanities - Data Science Initiative
# Associate Director - English Broadside Ballad Archive
#
# Portions of this code are based on Matt Jockers'
# Introduction to text analysis with R:
#
# Jockers, M. (2014).
# _Text Analysis with R for Students of Literature_
# Quantitative Methods in the Humanities and Social ….
# doi:10.1007/978-3-319-03164-4
#
# This work is licensed under a Creative Commons
# Attribution-ShareAlike 4.0 International License.
#
# see http://creativecommons.org/licenses/by-sa/4.0/
# install.packages("rJava")
# install.packages("NLP")
# install.packages("openNLP")
# install.packages("RWeka")
# install.packages("stats")
# install.packages("openNLPmodels.en", repos = "http://datacube.wu.ac.at/", type = "source")
# For alternate language openNLPModels see http://datacube.wu.ac.at/src/contrib/
library(rJava)
library(NLP)
library(openNLP)
library(RWeka)
library(stats)
###################################
#         configuration           #
###################################
# set working directory
setwd("~/Documents/rstudio_workspace/digitalmethods/text_mining/data/plainText")
# set the entity type you want to extract.  Legal
# values are:
#     date
#     location
#     money
#     organization
#     percentage
#     person
#     misc [proper nouns deemed not to fit anther category]
var_entityType_string = "person"
# set the file path
var_filePath_string = "melville.txt"
###################################
#      function declarations      #
###################################
# A callable function that writes out the contents
# of a vector in human readable form.
function_show_vector <- function(var_file_name_vec) {
for(i in 1:length(var_file_name_vec)) {
cat(i, var_file_name_vec[i], "\n", sep=" ")
}
}
# A callable function that extracts entities
# of an identified kind from an
# AnnotatedPlainTextDocument
function_extractEntities <- function(obj_doc, var_kind_character) {
var_content_string <- obj_doc$content
obj_annotations <- annotations(obj_doc)[[1]]
if(hasArg(var_kind_character)) {
var_kindFeatures_list <- sapply(obj_annotations$features, `[[`, "kind")
var_content_string[obj_annotations[var_kindFeatures_list == var_kind_character]]
} else {
var_content_string[obj_annotations[obj_annotations$type == "entity"]]
}
}
# A callable function that checks to see
# if an annotated document has any matching
# entities
function_checkForEntities <- function(doc, kind) {
#s <- doc$content
a <- annotations(doc)[[1]]
boolean_return <- TRUE
if(hasArg(kind)) {
k <- sapply(a$features, `[[`, "kind")
temp <- a[k == kind]
size <- length(temp)
if (size < 1) {
boolean_return <- FALSE
}
} else {
boolean_return <- FALSE
}
return(boolean_return)
}
###################################
#        Operational Code         #
###################################
# load the file to be analized into a character vector
# the resulting vector will have as many elements as
# lines in the file with the contents of each line
# contained in a character vector.
text.vector <- readLines(var_filePath_string)
# collapse the vector of lines into a single character
# vector
text.vector <- paste(text.vector, collapse = " ")
# explicitly convert the text.string character to a
# string class.  Necessary because the NLP is written
# in java.
text.string <- as.String(text.vector)
# create the annotators.
sent_ann <- Maxent_Sent_Token_Annotator()
word_ann <- Maxent_Word_Token_Annotator()
my.ann <- Maxent_Entity_Annotator(kind = var_entityType_string)
# assemble the list of annotators to
# send to the annotate function
pipeline.list <- list(sent_ann, word_ann, my.ann)
# create the final annotator model
text.annotator <- annotate(text.string, pipeline.list)
# create an annotated doc.  This is a version of the document that is
# represented as a structured hierarchy of sentences and words
text.doc <- AnnotatedPlainTextDocument(text.string, text.annotator)
# check to see if there are any matching entities.
# If so, process. If not, extit.
if (function_checkForEntities(text.doc, kind = var_entityType_string)) {
# get all entities of the type we are looking for
# debugonce(function_extractEntities)
foundEntities.v <- function_extractEntities(text.doc, var_kind_character = var_entityType_string)
# get vector of unique items
uniqueEntities.v <- unique(foundEntities.v)
# sort the entities vector
sortedEntities.v <- sort(uniqueEntities.v)
print("Sorted entity list:")
function_show_vector(sortedEntities.v)
###################################
# Code below is for cleaning list #
###################################
# review list of returned entities and create list
# of items entities that should be removed from the list
droplist.v <- c("With", "Starbuck", "Watts", "Plato")
# now remove items from the droplist from the vector of
# extracted named entities
finalNameList.v <- sortedEntities.v[! sortedEntities.v %in% droplist.v ]
print("Filtered entity list:")
function_show_vector(finalNameList.v)
} else {
print("No entities found")
}
